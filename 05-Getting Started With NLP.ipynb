
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with NLP\n",
    "\n",
    "Some common applications of NLP:\n",
    "\n",
    "* Text Summarization\n",
    "* Text Tagging (topic tagging)\n",
    "* Named Entity Recognition\n",
    "* Chatbot\n",
    "* Speech Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some terms:\n",
    "\n",
    "* Phonetics/phonology:\n",
    "The study of linguistic sounds and their relations to written words\n",
    "* Morphology:\n",
    "The study of internal structures of words/composition of words\n",
    "* Syntax:\n",
    "The study of the structural relationships among words in a sentence\n",
    "* Semantics:\n",
    "The study of the meaning of words and how these combine to form the meaning of sentences\n",
    "* Pragmatics:\n",
    "Situational use of language sentences\n",
    "* Discourse:\n",
    "A linguistic unit that is larger than a single sentence (context)\n",
    "* Tokenization:\n",
    "A step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words.\n",
    "* Normalization:\n",
    "Before further processing, text needs to be normalized. Normalization generally refers to a series of related tasks meant to put all text on a level playing field: converting all text to the same case (upper or lower), removing punctuation, expanding contractions, converting numbers to their word equivalents, and so on. Normalization puts all words on equal footing, and allows processing to proceed uniformly.\n",
    "* Stemming:\n",
    "is the process of eliminating affixes (suffixed, prefixes, infixes, postfix, circumfixes) from a word in order to obtain a word stem. ex: running → run\n",
    "* Lemmatization:\n",
    "lemmatization is able to capture canonical forms based on a word's lemma.\n",
    "For example, stemming the word \"better\" would fail to return its citation form (another word for lemma); however, lemmatization would result in the following:\n",
    "better → good\n",
    "* Corpus:\n",
    "(literally Latin for body) refers to a collection of texts. Such collections may be formed of a single language of texts, or can span multiple languages.\n",
    "* Stop words:\n",
    "words which are filtered out before further processing of text, since these words contribute little to overall meaning, given that they are generally the most common words in a language. For instance, \"the,\" \"and,\" and \"a,\" while all required words in a particular passage, don't generally contribute greatly to one's understanding of content.\n",
    "* Parts-of-speech (POS) Tagging:\n",
    "POS tagging consists of assigning a category tag to the tokenized parts of a sentence. The most popular POS tagging would be identifying words as nouns, verbs, adjectives, etc.\n",
    "* Bag of Words:\n",
    "representation model used to simplify the contents of a selection of text. The bag of words model omits grammar and word order, but is interested in the number of occurrences of words within the text. The ultimate representation of the text selection is that of a bag of words.\n",
    "\n",
    "  Ex:\n",
    "  \"Well, well, well,\" said John.\n",
    "  \"There, there,\" said James. \"There, there.\"\n",
    "  \n",
    "  The resulting bag of words representation as a dictionary:\n",
    "     {\n",
    "      'well': 3,\n",
    "      'said': 2,\n",
    "      'john': 1,\n",
    "      'there': 4,\n",
    "      'james': 1\n",
    "     }\n",
    "     \n",
    "     \n",
    "* N-grams:\n",
    "representation model for simplifying text selection contents. As opposed to the orderless representation of bag of words, n-grams modeling is interested in preserving contiguous sequences of N items from the text selection.\n",
    "\n",
    "  An example of trigram (3-gram) model of the second sentence of the above example (\"There, there,\" said James.     \"There, there.\") appears as a list representation below:\n",
    "\n",
    "   [\n",
    "      \"there there said\",\n",
    "      \"there said james\",\n",
    "      \"said james there\",\n",
    "      \"james there there\",\n",
    "   ]\n",
    "   \n",
    "* Statistical Language Modeling:\n",
    "the process of building a statistical language model which is meant to provide an estimate of a natural language. For a sequence of input words, the model would assign a probability to the entire sequence, which contributes to the estimated likelihood of various possible sequences. This can be especially useful for NLP applications which generate text.\n",
    "\n",
    "* Syntactic Analysis:\n",
    "Also referred to as parsing, syntactic analysis is the task of analyzing strings as symbols, and ensuring their conformance to a established set of grammatical rules.\n",
    "\n",
    "* Semantic Analysis:\n",
    "Also known as meaning generation, semantic analysis is interested in determining the meaning of text selections (either character or word sequences). After an input selection of text is read and parsed (analyzed syntactically), the text selection can then be interpreted for meaning. Simply put, syntactic analysis is concerned with what words a text selection was made up of, while semantic analysis wants to know what the collection of words actually means. \n",
    "\n",
    "* Sentiment analysis:\n",
    "the process of evaluating and determining the sentiment captured in a selection of text, with sentiment defined as feeling or emotion.\n",
    "   \n",
    "* Entity recognition:\n",
    "is the process used to classify multiple entities found in a text in predefined categories, such as a person,\n",
    "objects, location, organizations, dates, events, etc.\n",
    "\n",
    "* Word vector:\n",
    "refers to the mapping of the words or phrases from vocabulary to a vector of real numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Virtual Environment\n",
    "The main purpose of Python virtual environments is to create an isolated environment for Python projects. This means that each project can have its own dependencies, regardless of what dependencies every other project has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In python 2\n",
    "!pip install virtualenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you are using Python 3, then you should already have the venv module from the standard library installed.\n",
    "# To create a new virtual environment inside the directory\n",
    "# python3 -m venv (env_name)\n",
    "!python3 -m venv handsonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Debian/Ubuntu systems, you need to install the python3-venv package using the following command.\n",
    "# sudo apt-get install python3-venv\n",
    "# since it's not installed by default"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activate / Deactivate your environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!source handsonnlp/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!deactivate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List Installed Packages With Pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Natural Language Processing Libraries\n",
    "## 1. NLTK  (www.nltk.org/)\n",
    "Most common package you will encounter working with corpora, categorizing text, analyzing linguistic structure, and more.\n",
    "\n",
    "It was developed by Steven Bird and Edward Loper in the Department of Computer and Information Science at the University of Pennsylvania."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install NLTK with pip\n",
    "# activate your env first\n",
    "#!source handsonnlp/bin/activate\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},